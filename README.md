#WEB SCRAPING AND DATA VISUALISATION

Background: You are a data analyst working for a market research firm. Your company specializes in analyzing trends in e-commerce sales across various product categories. One of your clients, a leading retail chain, wants to understand the pricing dynamics of competitors in the consumer electronics category. They are particularly interested in the pricing trends of smartphones across different online platforms.

#TASK: 
Our Task was to develop a web scraping script using JavaScript and Selenium that collects smartphone pricing information from several online retailers. Once the data is collected, we will use jQuery to visualize the pricing trends.

Requirements:

#Web Scraping:

> Utilize Selenium WebDriver to scrape pricing data from at least three online retailers.
> The data should include the name of the smartphone, its price, and the date/time when the data was scraped.
> Implement error handling for timeouts, page loading issues, and element locating failures.

#Data Visualization:

> Use jQuery to create interactive visualizations that showcase the pricing trends of smartphones over time.
> Visualizations should include line charts or bar charts showing the price fluctuations of popular smartphone models across different retailers.
> Implement functionalities such as zooming, panning, and tooltip display for enhanced user experience.
> Ensure that the visualizations are clear, intuitive, and informative.

#Steps for solving the Task

Step 1: Clone the Repository

Clone the repository from GitHub:
> git clone https://github.com/Annemarie535257/alu-web-scraping.git

Step 2: Navigate to the Tests Directory
Navigate to the tests directory of the cloned repository:
> cd alu-web-scraping

Step 3: Test Scripts
Run the test scripts for Jumia:
> node jumia.js

If you encounter any errors during this step, please ensure that you have Node.js installed on your system. You can install Node.js and its dependencies by following these steps:

Install Node.js: Download Node.js from this URL

Install dependencies:
> npm install

Step 4: Run index.html

After running the test scripts successfully, you can run the index.html file to visualize the scraped data.

Made by Breakout Room 2

CONTRIBUTORS:
-Afsa Umutoniwase
-Anne Marie Twagirayezu
-Daniel Iryivuze
-Clarisse Tuyishimire
-Lina Iratwe
-Khalid Abdillahi
-Diana Ruzindana
-Jolly Umilisa
